{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f7c2f-1a05-4e49-ab98-7f256ec94e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "attr_path = \"/srv/stemly/data/7/index/3/attributes/attributes.csv\"\n",
    "index_path = \"/srv/stemly/data/7/index/3/index/index.csv\"\n",
    "committed_files = [\"/srv/stemly/data/7/index/3/timeseries/993/commit/ts.0/13.1661315396541.parquet\",\n",
    "                   \"/srv/stemly/data/7/index/3/timeseries/993/commit/ts.0/13.1662444054358.parquet\",\n",
    "                   \"/srv/stemly/data/7/index/3/timeseries/993/commit/ts.0/13.1662553683649.parquet\"]\n",
    "predict_ts_path = \"/srv/stemly/data/7/index/3/timeseries/1434/staged/model/16/ts.predict.latest.parquet\"\n",
    "test_ts_path = \"/srv/stemly/data/7/index/3/timeseries/1434/staged/model/16/ts.test.latest.parquet\"\n",
    "validate_ts_path = \"/srv/stemly/data/7/index/3/timeseries/1434/staged/model/16/ts.validate.latest.parquet\"\n",
    "model = \"custom\"\n",
    "model_file_id = \"32\"\n",
    "model_resolution = None\n",
    "reference_resolution = \"MS\"\n",
    "project_offset = 1\n",
    "project_periods = 18\n",
    "project_resolution = \"MS\"\n",
    "ts_train_start = \"2020-07-01 00:00:00\"\n",
    "ts_train_v_end = \"2022-05-01 00:00:00\"\n",
    "ts_train_t_end = \"2022-04-01 00:00:00\"\n",
    "ts_train_p_end = \"2022-06-01 00:00:00\"\n",
    "ts_test_start = \"2022-04-01 00:00:00\"\n",
    "ts_test_end = \"2022-05-01 00:00:00\"\n",
    "ts_test_duration = 1\n",
    "ts_validate_start = \"2022-05-01 00:00:00\"\n",
    "ts_validate_end = \"2022-06-01 00:00:00\"\n",
    "ts_validate_duration = 1\n",
    "ts_predict_start = \"2022-06-01 00:00:00\"\n",
    "ts_predict_end = \"2023-12-01 00:00:00\"\n",
    "ts_predict_duration = 18\n",
    "redis_url = \"redis://127.0.0.1:6300\"\n",
    "job_id = \"6bf40e7f-aced-40f7-86fe-f49f8d23b343\"\n",
    "job_user_id = \"3\"\n",
    "resource_id = \"16\"\n",
    "resource_version = 1661403479559\n",
    "resource_type = \"modelbase\"\n",
    "workspace_id = \"7\"\n",
    "job_name = \"custom\"\n",
    "job_triggered_at = \"2022-08-25 04:58:00\"\n",
    "job_triggered_by = \"Mallikarjuna Reddy Padigapati\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84519908-8d23-495e-b61e-d5d76320d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm as tqdm \n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de41cb1-280b-40e1-8f00-12e9865b6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = 'w' + workspace_id\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef52882-3f7b-4c87-90da-9f85ed334eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_group_index = ['sku', 'customer', 'warehouse']\n",
    "mat_group = 'sku'\n",
    "freq = \"MS\"\n",
    "load_data_from_local = False\n",
    "export_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1ec5e-99e1-48f9-b8f6-ae48d12559d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_index = 1103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb496947-4754-4673-ab9e-cedd44df73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_data_from_local :\n",
    "    price_var = 'price'\n",
    "    volume_var = 'dc_offtake'\n",
    "    main_group_index = ['sku', 'customer', 'warehouse']\n",
    "    mat_group = 'sku'\n",
    "else :\n",
    "    price_var = 'y'\n",
    "    volume_var = 'y'\n",
    "    main_group_index = ['id']\n",
    "    mat_group = 'sku'\n",
    "    \n",
    "joining_keys = [\"ts\"] + main_group_index\n",
    "joining_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd52504-50c8-41ef-878b-676b673a18ed",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7b586-bf10-4676-afe7-1f6f346b03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_data_from_local :\n",
    "    att_df = pd.read_csv(\"../../Cleaned_Input/all_att_data.csv\")\n",
    "    vol_df = pd.read_parquet(\"../../Cleaned_Input/UI_Files/si_monthly_volume.parquet\")\n",
    "    price_df = pd.read_parquet(\"../../Cleaned_Input/UI_Files/si_monthly_price.parquet\")\n",
    "    list_price_df = pd.read_parquet(\"../../Cleaned_Input/UI_Files/si_monthly_price.parquet\")\n",
    "    \n",
    "    vol_df.rename(columns={'date':'ts'},inplace=True)\n",
    "    price_df.rename(columns={'date':'ts'},inplace=True)\n",
    "    list_price_df.rename(columns={'date':'ts'},inplace=True)\n",
    "\n",
    "if not load_data_from_local:\n",
    "    path_index_val = os.listdir(\"/srv/stemly/data/{}/index\".format(workspace_id))[0]\n",
    "    path_index = \"/srv/stemly/data/{}/index\".format(workspace_id) + \"/\" + str(path_index_val)\n",
    "    vol_df = pd.concat([pd.read_parquet(fp) for fp in committed_files])\n",
    "\n",
    "    att_df = pd.read_csv(attr_path)\n",
    "    price_parquet_file_path = path_index + \"/timeseries/\"+ str(price_index)+ \"/commit/ts.0\"\n",
    "    price_file_names =  os.listdir(price_parquet_file_path)\n",
    "    price_file_paths = [price_parquet_file_path+ \"/\" + i for i in price_file_names]\n",
    "    print(price_file_paths)\n",
    "\n",
    "    price_df = pd.concat([pd.read_parquet(fp) for fp in price_file_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525447ce-3a01-40c7-9e6b-d341ff578a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_group_index = ['ts','sku','customer']\n",
    "att_df_sub = att_df[[ 'sku', 'customer', 'item_type', 'material', 'ph_level2', 'ph_level3',\n",
    "       'sku_description', 'cases','uom']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec8c49-3f4d-4142-94d3-2d2e512602ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df.rename(columns={volume_var:'y'},inplace=True)\n",
    "price_df.rename(columns={price_var:'price'},inplace=True)\n",
    "\n",
    "price_df = price_df[main_group_index+['price','ts']]\n",
    "vol_df = vol_df[main_group_index+['y','ts']]\n",
    "\n",
    "vol_df = vol_df.groupby(joining_keys).sum().reset_index()\n",
    "price_df = price_df.groupby(joining_keys).mean().reset_index()\n",
    "if not load_data_from_local:\n",
    "    att_df.rename(columns={'__idx':'id'},inplace=True)\n",
    "\n",
    "print(vol_df.shape)\n",
    "df_base = pd.merge(vol_df, price_df, on =joining_keys, how='left')\n",
    "df_base = pd.merge(df_base, att_df,  on = main_group_index, how='left')\n",
    "df_base\n",
    "df_base.loc[df_base['y'] < 0,'y'] = 0\n",
    "print(df_base.shape)\n",
    "\n",
    "df_sub = df_base[['ts','sku','customer','y','price']]\n",
    "df_sub1 = df_sub.groupby(sub_group_index).agg({'y':'sum','price':'mean'}).reset_index()\n",
    "df_sub1 = pd.merge(df_sub1,att_df_sub,on=['sku','customer'],how='left')\n",
    "df_sub1['price'].replace([np.inf,0, -np.inf],np.nan,inplace=True)\n",
    "df_sub1['price'] = df_sub1.groupby(sub_group_index)['price'].transform(lambda x: x.ffill())\n",
    "df_sub1['price'] = df_sub1.groupby(sub_group_index)['price'].transform(lambda x: x.bfill())\n",
    "df_sub1['price'].replace([np.nan],0,inplace=True)\n",
    "\n",
    "df_main = df_sub1.copy()\n",
    "df_sub1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df279104-1fa2-414a-b0da-098f0d808235",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_main.copy()\n",
    "data.head()\n",
    "\n",
    "unique_id = data[['sku','customer']].drop_duplicates().reset_index(drop=True)\n",
    "unique_id['id'] = unique_id.index\n",
    "data = pd.merge(data,unique_id,on=['sku','customer'])\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "def to_days(x):\n",
    "    switcher = {0: \"Mon\", 1: \"Tue\", 2: \"Wed\", 3: \"Thu\", 4: \"Fri\", 5: \"Sat\", 6: \"Sun\"}\n",
    "    return switcher.get(x)\n",
    "def week_of_month(date_value):\n",
    "    return (date_value.isocalendar()[1] - date_value.replace(day=1).isocalendar()[1] + 1)\n",
    "data[\"month\"] = data[\"ts\"].dt.month \n",
    "data[\"year\"] = data[\"ts\"].dt.year \n",
    "\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "id_sales_train = data[data['ts'] <= ts_predict_start]\n",
    "id_sales_train = id_sales_train.groupby('id')['y'].sum().reset_index()\n",
    "id_with_sales_df = id_sales_train[id_sales_train['y'] > 0]\n",
    "id_with_sales = id_with_sales_df.id.unique()\n",
    "data = data[(data['id'].isin(id_with_sales))]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959e5b8-d070-4359-9fed-664b98a4291f",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d4f4a-40c4-434d-b7fd-55f69381a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def r2_rmse(g, actual=\"y\", prediction=\"yhat\"):\n",
    "    r2 = r2_score( g[actual], g[prediction] )\n",
    "    rmse = np.sqrt( mean_squared_error( g[actual], g[prediction] ) )\n",
    "    actuals = g[actual].sum()\n",
    "    predicted = g[prediction].sum()\n",
    "    return pd.Series( dict(  actuals = actuals,prediction = predicted,r2 = r2, rmse = rmse) )\n",
    "\n",
    "def mape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "\n",
    "def mean_absolute_percentage_error(g, actual='y', prediction='yhat'): \n",
    "    y_true, y_pred = np.array(g[actual]), np.array(g[prediction])\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def mase(g, actual='y', prediction='yhat'):\n",
    "    \"\"\"\n",
    "    Computes the MEAN-ABSOLUTE SCALED ERROR forcast error for univariate time series prediction.\n",
    "    \n",
    "    See \"Another look at measures of forecast accuracy\", Rob J Hyndman\n",
    "    \n",
    "    parameters:\n",
    "        training_series: the series used to train the model, 1d numpy array\n",
    "        testing_series: the test series to predict, 1d numpy array or float\n",
    "        prediction_series: the prediction of testing_series, 1d numpy array (same size as testing_series) or float\n",
    "        absolute: \"squares\" to use sum of squares and root the result, \"absolute\" to use absolute values.\n",
    "    \n",
    "    \"\"\"\n",
    "    n = g.shape[0]\n",
    "    d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "    \n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d\n",
    "\n",
    "def ape(g, actual='y', prediction='yhat'):\n",
    "    ape_vals = abs(g[actual] - g[prediction]) / g[actual]\n",
    "    ape_vals = ape_vals*100\n",
    "    ape_vals = ape_vals.round(2)  \n",
    "    return ape_vals\n",
    "\n",
    "def extract_sliding(df, window=8, gap=4, target=\"y\"):\n",
    "    lags = [x for x in range(gap + 1, gap + window + 1)]\n",
    "    aa = df[[\"id\", \"ts\", \"y\"]].assign(**{\n",
    "        \"{}_lag_{}\".format(col, l): df.groupby([\"id\"], observed=True)[col].transform(lambda x: x.shift(l))\n",
    "        for l in lags\n",
    "        for col in [target]\n",
    "    })\n",
    "    aa.sort_values(by=[\"id\", \"ts\"], inplace=True)\n",
    "    aa.reset_index(inplace=True)\n",
    "    aa.rename(columns={\"index\": \"sub_id\"}, inplace=True)\n",
    "\n",
    "    b = aa.drop(columns=[\"id\", \"ts\", \"y\"]).melt(\n",
    "        id_vars=\"sub_id\",\n",
    "        value_name=\"y\",\n",
    "        var_name=\"lags\"\n",
    "    )\n",
    "    b[\"lags\"] = [int(x.replace(\"y_lag_\", \"\")) for x in b[\"lags\"]]\n",
    "    return aa, b\n",
    "\n",
    "def calc_metrics(y, yhat):\n",
    "    \"\"\"\n",
    "    Calculate metrics for a single series.\n",
    "    \"\"\"\n",
    "    # mae\n",
    "    mae = np.mean(np.abs(y - yhat))\n",
    "\n",
    "    # smape\n",
    "    smape = 2 * np.abs((y - yhat) / (y + yhat))\n",
    "    smape = np.nan_to_num(smape, 0)\n",
    "    smape = np.mean(smape)\n",
    "\n",
    "    # mape\n",
    "    mape = np.abs((y - yhat) / y)\n",
    "    mape = np.nan_to_num(mape, 0)\n",
    "    mape = np.mean(mape)\n",
    "\n",
    "    # maape\n",
    "    maape = np.arctan2(np.abs(y - yhat), np.abs(y))\n",
    "    maape = np.mean(maape)\n",
    "\n",
    "    # rmse\n",
    "    rmse = (y - yhat) * (y - yhat)\n",
    "    rmse = np.sqrt(np.mean(rmse))\n",
    "\n",
    "    return {\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"smape\": smape,\n",
    "        \"mape\": mape,\n",
    "        \"maape\": maape\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24010bd-ea6d-4bfc-afae-bcf2f0d4773f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1803a0-c263-4f5f-9524-130dc3b04989",
   "metadata": {},
   "source": [
    "### Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717db543-2517-438e-961a-35bed7e8309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"MS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6dc41-7296-4161-b8ea-f7c6317f96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "# df[\"year\"] = df.ts.dt.year\n",
    "df[\"month\"] = df.ts.dt.month\n",
    "df.dropna(subset=[\"y\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df[ df['customer'] == 'TMALL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0681d63-d03f-4465-8cfd-87c9f5629196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f89da8-3e42-4b33-8b13-3b0606c22f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "def generate_dates_new(\n",
    "        last_test_date=\"2021-12-27\",\n",
    "        gap_in_days=28,\n",
    "        test_horizon_in_days=28,\n",
    "        version_skip = 28,\n",
    "        fc_horizon_in_days=84,\n",
    "        freq=\"W-Mon\",\n",
    "        n_rounds=2\n",
    "):\n",
    "    ts_test_end = pd.to_datetime(last_test_date) \n",
    "    ts_test_start = ts_test_end - pd.Timedelta(days=test_horizon_in_days - 1)\n",
    "   \n",
    "    ts_predict_start = ts_test_end + pd.Timedelta(days=1)\n",
    "    ts_predict_end = ts_predict_start + pd.Timedelta(days=fc_horizon_in_days-1)\n",
    "\n",
    "   \n",
    "    test_dates = pd.date_range(\n",
    "        start=ts_test_start,\n",
    "        end=ts_test_end,\n",
    "        freq=freq,\n",
    "    )\n",
    "    forecast_dates = pd.date_range(\n",
    "        start=ts_predict_start,\n",
    "        end=ts_predict_end,\n",
    "        freq=freq,\n",
    "    )\n",
    "    all_dates1 = df.ts.unique()\n",
    "    train_dates = all_dates1[all_dates1 < np.min(test_dates)]\n",
    "    a = test_dates[0].month\n",
    "    all_dates_new = {\n",
    "        a: {\n",
    "            \n",
    "            \"train\" : pd.to_datetime(train_dates),\n",
    "            \"test\": test_dates,\n",
    "            \"forecast\": forecast_dates,\n",
    "        },\n",
    "    }\n",
    "    for k in range(n_rounds):\n",
    "        a = a - 1\n",
    "        if a == 0:\n",
    "            a = 12\n",
    "        train_dates = pd.date_range(\n",
    "            start=pd.to_datetime(train_dates[0]),\n",
    "            end=pd.to_datetime(train_dates[-1]) - relativedelta(months=version_skip),\n",
    "            freq=freq\n",
    "        )\n",
    "        test_dates = pd.date_range(\n",
    "            start=test_dates[0]- relativedelta(months=version_skip),\n",
    "            end=test_dates[-1] - relativedelta(months=version_skip),\n",
    "            freq=freq\n",
    "        )\n",
    "        forecast_dates = pd.date_range(\n",
    "            start=forecast_dates[0]- relativedelta(months=version_skip),\n",
    "            end=forecast_dates[-1] - relativedelta(months=version_skip),\n",
    "            freq=freq\n",
    "        )\n",
    "        all_dates_new.update({\n",
    "            a: {\n",
    "                \"train\" : train_dates,\n",
    "                \"test\": test_dates,\n",
    "                \"forecast\": forecast_dates,\n",
    "            }\n",
    "        })\n",
    "    return all_dates_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54f4f3-e336-45e1-ab95-4d9332de94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_test_end = \"2022-08-01 00:00:00\"\n",
    "all_dates = generate_dates_new(\n",
    "#     last_test_date=\"2021-12-27\",\n",
    "    last_test_date= ts_test_end,\n",
    "    freq=freq,\n",
    "    gap_in_days=30,\n",
    "    version_skip = 1,\n",
    "    fc_horizon_in_days = 367,\n",
    "    test_horizon_in_days=30,\n",
    "    n_rounds=2\n",
    ")\n",
    "all_dates\n",
    "\n",
    "versions = []\n",
    "for i in all_dates.keys():\n",
    "    versions.append(i)\n",
    "m=list(all_dates.keys())[0]\n",
    "    \n",
    "versions ,all_dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b93f5-293c-4a11-a0ef-d596734a5268",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for a in df.sku.unique():\n",
    "    ts = df.loc[df.sku == a].groupby(\"ts\").sum()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(ts.y)\n",
    "    plt.title(a)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8aed1-5e77-4a16-b4ac-baddb2965108",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for a in df.sku.unique():\n",
    "    ts = df.loc[df.sku == a].groupby(\"ts\").sum()\n",
    "    a = ts.loc[ts.index < \"2022-06-01\"]\n",
    "    b = ts.loc[ts.index >= \"2022-06-01\"]\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.bar(a.index, a.y, width=5)\n",
    "    plt.bar(b.index, b.y, width=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a84f9-e8b1-43bb-9b9d-a04a59e4a81f",
   "metadata": {},
   "source": [
    "### Differenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a30627-0c84-4acd-bb48-4ce88b0fc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = []\n",
    "all_ids = df.id.unique()\n",
    "initial = []\n",
    "for i in range(len(all_ids)):\n",
    "    id_ = all_ids[i]\n",
    "    tmp = df.loc[df.id == id_].reset_index(drop=True)\n",
    "    tmp2 = tmp.copy()\n",
    "    initial.append(tmp[\"y\"][0])\n",
    "    \n",
    "    tmp2[\"y\"] = tmp[\"y\"].diff()\n",
    "    #tmp2[\"y\"].fillna(tmp[\"y\"], inplace=True)\n",
    "    tmp2.dropna(subset=\"y\", inplace=True)\n",
    "    df_diff.append(tmp2)\n",
    "initial_y = pd.DataFrame({\"id\": all_ids, \"initial\": initial})\n",
    "initial_y.set_index(\"id\", inplace=True)\n",
    "df_diff = pd.concat(df_diff).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d8fa9-07db-45ff-a114-3cf4bf048ff1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for a in df_diff.id.unique():\n",
    "    ts = df_diff.loc[df_diff.id == a].groupby(\"ts\").sum()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(ts.y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489d443-2cc0-4c57-a76b-24f3e970838f",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34ab00-879d-4729-827f-94890a1ce3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab79e6-daf5-47ae-899f-44ae1c2d148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_cols = [\"id\", \"sku_description\", \"sku\", \"item_type\", \"uom\", \"cases\", \"ph_level2\"]\n",
    "att = df[att_cols].drop_duplicates()\n",
    "att.reset_index(inplace=True, drop=True)\n",
    "print(\"Att\", att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d57673-ff1c-48c8-ae20-6efa6ea1b586",
   "metadata": {},
   "source": [
    "## Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec167f0-fcf6-46c9-ad77-0a106993e629",
   "metadata": {},
   "source": [
    "## HW-ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afd230-bc26-4094-89dc-4dc7ed310cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.exponential_smoothing.ets import ETSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67218779-1df5-4da3-bc9a-078d2082331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d821b-1c66-46ab-941f-d5b3d92b0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ets_forecast(\n",
    "    series,\n",
    "    train_dates,\n",
    "    test_dates,\n",
    "):    \n",
    "    if len(series) > 0:\n",
    "        # time series must not be empty!\n",
    "\n",
    "        # fit parameters on val\n",
    "        train = series.loc[series.index.isin(train_dates)]\n",
    "        test = series.loc[series.index.isin(test_dates)]\n",
    "            \n",
    "        if np.sum(train) == 0:\n",
    "            # special case when all the values are 0\n",
    "            out = np.zeros(len(train)+len(test))\n",
    "            \n",
    "            return out\n",
    "        if len(train) == 0:\n",
    "            # special case when all the values are 0\n",
    "            out = np.zeros(len(train)+len(test))\n",
    "            \n",
    "            return out\n",
    "\n",
    "        # do cross validation\n",
    "        configs = [True, False]\n",
    "        error = [\"add\"]\n",
    "        trend = [\"add\"]\n",
    "        damped_trend = [True, False]\n",
    "        seasonal = [\"add\"]\n",
    "        seasonal_periods = [4,6,12]\n",
    "        best_config = None\n",
    "        best_metric = np.inf\n",
    "        best_quantile = None\n",
    "        best_model = None\n",
    "        for q in [0.95, 0.96, 0.97, 0.98, 0.99]:\n",
    "            for e in error:\n",
    "                for t in trend:\n",
    "                    for d in damped_trend:\n",
    "                        for s in seasonal:\n",
    "                            if s is None:\n",
    "                                model = ETSModel(\n",
    "                                    np.clip(train, 0, np.quantile(train[np.where(train > 0)[0][0]:], q)),\n",
    "                                    error=e,\n",
    "                                    trend=t,\n",
    "                                    seasonal=s,\n",
    "                                    damped_trend=d,\n",
    "                                    seasonal_periods=None,\n",
    "                                ).fit()\n",
    "                                \n",
    "                                yhat = model.predict(0, len(train) - 1)\n",
    "                                yhat = np.array(yhat)\n",
    "                                yhat = np.clip(yhat, 0, None)\n",
    "                                \n",
    "                                rmse = (train.values - yhat) * (train.values - yhat)\n",
    "                                rmse = np.sqrt(np.mean(rmse))\n",
    "\n",
    "                                if rmse < best_metric:\n",
    "                                    best_metric = rmse\n",
    "                                    best_model = model\n",
    "                            else:\n",
    "                                for ss in seasonal_periods:\n",
    "                                    try:\n",
    "                                        model = ETSModel(\n",
    "                                            np.clip(train, 0, np.quantile(train[np.where(train > 0)[0][0]:], q)),\n",
    "                                            error=e,\n",
    "                                            trend=t,\n",
    "                                            seasonal=s,\n",
    "                                            damped_trend=d,\n",
    "                                            seasonal_periods=ss,\n",
    "                                        ).fit()\n",
    "                                    except:\n",
    "                                        model = ETSModel(\n",
    "                                            np.clip(train, 0, np.quantile(train[np.where(train > 0)[0][0]:], q)),\n",
    "                                            error=e,\n",
    "                                            trend=t,\n",
    "                                            seasonal=None,\n",
    "                                            damped_trend=d,\n",
    "                                            seasonal_periods=None,\n",
    "                                        ).fit()\n",
    "\n",
    "                                    yhat = model.predict(0, len(train) - 1)\n",
    "                                    yhat = np.array(yhat)\n",
    "                                    yhat = np.clip(yhat, 0, None)\n",
    "\n",
    "                                    rmse = (train.values - yhat) * (train.values - yhat)\n",
    "                                    rmse = np.sqrt(np.mean(rmse))\n",
    "\n",
    "                                    if rmse < best_metric:\n",
    "                                        best_metric = rmse\n",
    "                                        best_model = model\n",
    "\n",
    "        # do forecast on train\n",
    "        yhat_train = best_model.predict(0, len(train) - 1)\n",
    "        yhat_train = np.array(yhat_train)\n",
    "        yhat_train = np.clip(yhat_train, 0, None)\n",
    "        \n",
    "        # first forecast on test to get a sense of how well the model performs\n",
    "        yhat_test = best_model.forecast(len(test))\n",
    "        yhat_test = np.array(yhat_test)\n",
    "        yhat_test = np.clip(yhat_test, 0, None)\n",
    "        \n",
    "        # concatenate all predictions\n",
    "        out = np.concatenate((yhat_train, yhat_test))\n",
    "\n",
    "        return out\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d2a33-e51c-4f69-9695-674a8acfff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_ets(\n",
    "    data,\n",
    "    dates,\n",
    "    num_cores=-1,\n",
    "    freq=\"MS\",\n",
    "):\n",
    "    import multiprocessing\n",
    "    from pandarallel import pandarallel\n",
    "    \n",
    "    import warnings\n",
    "    import logging\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "    logging.getLogger('prophet').setLevel(logging.ERROR)\n",
    "    \n",
    "    if num_cores < 0:\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "    else:\n",
    "        num_cores = min(num_cores, multiprocessing.cpu_count())\n",
    "        \n",
    "    pandarallel.initialize(\n",
    "        nb_workers=num_cores,\n",
    "        verbose=0,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    train_dates = dates[\"train\"]\n",
    "    test_dates = dates[\"test\"]\n",
    "    \n",
    "    s = data.set_index([\"id\", \"ts\"], drop=True)[\"y\"]\n",
    "    s = s.unstack(0).fillna(0)\n",
    "    s = s.asfreq(freq)\n",
    "    \n",
    "    print(train_dates)\n",
    "    print(test_dates)\n",
    "    yhat = s.parallel_apply(lambda x: ets_forecast(\n",
    "        x,\n",
    "        train_dates,\n",
    "        test_dates,\n",
    "    ), axis=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    yhat = yhat.reset_index(drop=True).apply(lambda x: pd.Series(x.dropna().values)).dropna()\n",
    "\n",
    "    yhat[\"ts\"] = pd.date_range(\n",
    "        start=max(np.min(train_dates), data.ts.min()),\n",
    "        end=np.max(test_dates),\n",
    "        freq=freq\n",
    "    )\n",
    "    \n",
    "    yhat = pd.melt(yhat, id_vars=['ts'], var_name=\"id\", value_name=\"yhat\")\n",
    "    yhat = pd.merge(df, yhat, on=[\"ts\", \"id\"], how=\"left\")\n",
    "    \n",
    "    yhat = yhat[[\"id\", \"ts\", \"y\", \"yhat\"]+[x for x in yhat.columns if x not in [\"id\", \"ts\", \"y\", \"yhat\"]]]\n",
    "          \n",
    "    return yhat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca86354-5867-45c1-a39c-c16a462944b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_hws = {}\n",
    "for i in range(len(versions)):\n",
    "    pred_hw = forecast_ets(\n",
    "        df,\n",
    "        all_dates[versions[i]],\n",
    "        num_cores=-1,\n",
    "        freq=freq,\n",
    "    )\n",
    "    pred_hws.update({versions[i]: pred_hw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320a959-e66f-471b-800a-015b71515460",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_hws = {}\n",
    "for i in range(len(versions)):\n",
    "    yhat_hw = pred_hws[versions[i]]\n",
    "    yhat_hws.update({versions[i]: yhat_hw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4a516-7d8d-4044-9e0b-66f2381708ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(versions)):\n",
    "    yhat_hw = yhat_hws[versions[i]]\n",
    "    ts = yhat_hw.groupby(\"ts\")[[\"y\", \"yhat\"]].sum()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(ts.y, label=\"actual\")\n",
    "    plt.plot(ts.yhat, label=\"forecast\")\n",
    "    plt.title(\"version {}\".format(versions[i]))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcacaec-92f4-4759-9c6a-18514d166776",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(versions)):\n",
    "    yhat_hw = yhat_hws[versions[i]]\n",
    "    ts = yhat_hw.groupby([\"sku\", \"ts\"])[[\"y\", \"yhat\"]].sum()\n",
    "    ts.reset_index(inplace=True)\n",
    "\n",
    "    for this_sku in ts.sku.unique():\n",
    "        _ts = ts.loc[ts.sku == this_sku]\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(_ts.ts, _ts.y)\n",
    "        plt.plot(_ts.ts, _ts.yhat)\n",
    "        plt.title(str(this_sku) + \" Version {}\".format(versions[i]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89633a2-e3ae-49b1-bb81-3dcc54f00f77",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09431b-36a2-45a5-aaf4-42121595846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(versions)):\n",
    "    pred_hw = pred_hws[versions[i]]\n",
    "    for _id in pred_hw.id.unique():\n",
    "        ts = pred_hw.loc[(pred_hw[\"id\"] == _id) ].reset_index(drop=True)\n",
    "        test = ts.loc[ts.ts.isin(all_dates[versions[i]]['test'])].copy()\n",
    "\n",
    "        metrics = calc_metrics(test.y, test.yhat)\n",
    "        metrics.update({\n",
    "            \"id\": _id,\n",
    "            \"sku\": ts[\"sku\"][0],\n",
    "            \"customer\": ts[\"customer\"][0],\n",
    "            \"version\" : versions[i],\n",
    "            \"test_start\": all_dates[versions[i]]['test'][0],\n",
    "            \"test_end\": all_dates[versions[i]]['test'][-1],\n",
    "            \"total_actual\": test.y.sum(),\n",
    "            \"total_forecast\": test.yhat.sum(),\n",
    "        })\n",
    "\n",
    "        results.append(pd.DataFrame(metrics, index=[0]))\n",
    "results = pd.concat(results).reset_index(drop=True)\n",
    "\n",
    "main_cols = [\"version\", \"id\", \"sku\", \"customer\", \"test_start\", \"test_end\", \"total_actual\", \"total_forecast\"]\n",
    "results = results[main_cols + list(results.drop(columns=main_cols).columns)].copy()\n",
    "results[\"sp1\"] = results[\"total_actual\"] / results[\"total_forecast\"]\n",
    "results[\"sp1\"] = np.nan_to_num(results[\"sp1\"], nan=1, posinf=0, neginf=0)\n",
    "\n",
    "results = results.sort_values(by=[\"version\",\"id\"]).reset_index(drop=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1270b6-3a37-4ae1-8935-67d4f224d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean(), results.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e220e-9ddd-4bef-9093-ef742ce72c0c",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03582e59-603d-4a67-a3c2-3666846eafb2",
   "metadata": {},
   "source": [
    "### Lag+Sliding window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2412b-d03e-4f6d-aafb-2e05ae630068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_features(ddf, gaps=None, target=\"y\", window=3):\n",
    "    stats = [\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"]\n",
    "    if gaps is None:\n",
    "        gaps = [1,2,3]\n",
    "    features = {}\n",
    "    for gap in gaps:\n",
    "        df = ddf.copy()\n",
    "        y = None\n",
    "        if (target != \"y\") and (target in df.columns):\n",
    "            y = df[[\"id\", \"ts\", \"y\"]].copy()\n",
    "            df.drop(columns=[\"y\"], inplace=True)\n",
    "            df[target].fillna(0, inplace=True)\n",
    "            df.rename(columns={target: \"y\"}, inplace=True)\n",
    "            \n",
    "        df_lags, df_lags_long = extract_sliding(df, gap=gap, window=window)\n",
    "       \n",
    "        df_lags.dropna(inplace=True)\n",
    "        if y is not None:\n",
    "            df_lags.rename(columns={\"y\": target}, inplace=True)\n",
    "            df_lags = pd.merge(y, df_lags, on=[\"id\", \"ts\"], how=\"right\")\n",
    "        \n",
    "            \n",
    "        lag_cols = [x for x in df_lags.columns if \"_lag\" in x]\n",
    "        sliding_stats = df_lags.copy()\n",
    "        for i in range(len(lag_cols)):\n",
    "            a = lag_cols[i]\n",
    "            for j in range(i+1, len(lag_cols)):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                b = lag_cols[j]\n",
    "                c = \"{}_{}_div_{}\".format(target, a.replace(\"y_\", \"\"), b.replace(\"y_\", \"\"))\n",
    "                sliding_stats[c] = sliding_stats[a] / sliding_stats[b]\n",
    "                sliding_stats[c] = np.nan_to_num(sliding_stats[c], False, 0, 0, 0)\n",
    "        for s in stats:\n",
    "            sliding_stats[\"sliding_{}_{}\".format(target, s)] = sliding_stats[lag_cols].agg(s, axis=1)\n",
    "        sliding_stats[\"{}_mean_div_median\".format(target)] = sliding_stats[\"sliding_{}_mean\".format(target)] / sliding_stats[\"sliding_{}_median\".format(target)]\n",
    "        sliding_stats[\"{}_mean_div_sum\".format(target)] = sliding_stats[\"sliding_{}_mean\".format(target)] / sliding_stats[\"sliding_{}_sum\".format(target)]\n",
    "        sliding_stats[\"{}_median_div_sum\".format(target)] = sliding_stats[\"sliding_{}_median\".format(target)] / sliding_stats[\"sliding_{}_sum\".format(target)]\n",
    "        \n",
    "        sliding_stats[\"{}_mean_div_median\".format(target)] = np.nan_to_num(sliding_stats[\"{}_mean_div_median\".format(target)], False, 0, 0, 0)\n",
    "        sliding_stats[\"{}_mean_div_sum\".format(target)] = np.nan_to_num(sliding_stats[\"{}_mean_div_sum\".format(target)], False, 0, 0, 0)\n",
    "        sliding_stats[\"{}_median_div_sum\".format(target)] = np.nan_to_num(sliding_stats[\"{}_median_div_sum\".format(target)], False, 0, 0, 0)\n",
    "        \n",
    "        sliding_stats.sort_values(by=[\"id\", \"ts\"], inplace=True)\n",
    "        sliding_stats.reset_index(drop=True, inplace=True)\n",
    "        sliding_stats.drop(columns=[\"sub_id\"], inplace=True)\n",
    "        for c in lag_cols:\n",
    "            sliding_stats.rename(columns={c: c.replace(\"y\", target)}, inplace=True)\n",
    "        features.update({gap: sliding_stats})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779f3ff-b913-4b99-ae15-3ea1566929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sliding_all = {}\n",
    "for i in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[i]]['forecast'])]\n",
    "    y_sliding = sliding_window_features(this_df, [0,1,2,3])\n",
    "    y_sliding_all.update({versions[i]: y_sliding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a91b3-62b4-454b-8c80-c36bd64425bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = y_sliding_all[m][3]\n",
    "print(tmp.shape)\n",
    "tmp.loc[tmp.id == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b1c82-2294-4d4f-b768-288aeec1d113",
   "metadata": {},
   "source": [
    "### Cluster - Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cb00f-b908-4dc6-9f4d-0988809b74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d1b72-3781-4fb3-80df-0f3c5a327529",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clust_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    x_clust = this_df.pivot_table(index=\"id\", columns=\"ts\", values=\"y\").fillna(0)\n",
    "    a = MinMaxScaler().fit_transform(x_clust.transpose()).transpose()\n",
    "    x_clust.loc[:, :] = a\n",
    "    x_clust_all.update({versions[this_version]: x_clust})\n",
    "\n",
    "x_clust_all[m].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd04a4-9722-49a1-8014-1da1fe6f5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    from scipy.cluster.hierarchy import dendrogram\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0ab5-0902-40ea-9000-954631c86eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "clust_model_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    x_clust = x_clust_all[versions[this_version]]\n",
    "    clust_model = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        linkage=\"ward\",\n",
    "        compute_distances=True\n",
    "    )\n",
    "    clust_model = clust_model.fit(x_clust)\n",
    "    clust_model_all.update({versions[this_version]: clust_model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3543b7-495c-4442-9185-bdc945903e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top three levels of the dendrogram\n",
    "plt.figure(figsize=(15,5))\n",
    "plot_dendrogram(clust_model_all[m], truncate_mode=\"level\", p=10)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec05022-d237-4cc1-96fb-aa107119617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = x_clust_all[m].index\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674c530-c8f3-43f2-b8a7-553595c0e9ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_average_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    y_sliding = sliding_window_features(this_df, [0,1,2,3])\n",
    "    clust_model = clust_model_all[versions[this_version]]\n",
    "    x_clust = x_clust_all[versions[this_version]]\n",
    "    cluster_average = []\n",
    "    all_ids = x_clust.index\n",
    "    for i in range(n_clusters):\n",
    "        idx = np.where(clust_model.labels_==i)\n",
    "        idx2 = all_ids[idx[0]]\n",
    "\n",
    "        _at = att.loc[att.id.isin(idx2)]\n",
    "        ts = x_clust.loc[idx2].values\n",
    "        t = x_clust.columns\n",
    "        \n",
    "        avg = ts.mean(axis=0)\n",
    "        cluster_average.append(pd.DataFrame({\"ts\": t, \"y\": avg, \"cluster\": i}))\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        for j in range(ts.shape[0]):\n",
    "            plt.plot(t, ts[j])\n",
    "        plt.plot(t, avg, \"k\", linewidth=5)\n",
    "        if ts.shape[0] == 1:\n",
    "            plt.title(\"Cluster {}: ID: {} Version: {}\".format(i, idx[0][0], versions[this_version]))\n",
    "        else:\n",
    "            plt.title(\"Cluster {}: {} Version: {}\".format(i, ts.shape[0],  versions[this_version]))\n",
    "        plt.show()\n",
    "    cluster_average = pd.concat(cluster_average).reset_index(drop=True)\n",
    "    cluster_average_all.update({versions[this_version]: cluster_average})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a26b9-709a-4df1-9c26-7a95e2192730",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    cluster_features = att.copy()\n",
    "    clust_model = clust_model_all[versions[this_version]]\n",
    "    cluster_features[\"cluster_norm\"] = clust_model.labels_\n",
    "    cluster_features = cluster_features[[\"id\", \"cluster_norm\"]].copy()\n",
    "    cluster_features_all.update({versions[this_version]: cluster_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064c8ed-11d3-48fa-b600-3d029873577f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "clust_avg_yhat_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    cluster_average = cluster_average_all[versions[this_version]]\n",
    "    clust_avg_yhat = forecast_ets(\n",
    "        cluster_average.rename(columns={\"cluster\": \"id\"}),\n",
    "        all_dates[versions[this_version]],\n",
    "        num_cores=-1,\n",
    "        freq=freq,\n",
    "    )\n",
    "    clust_avg_yhat_all.update({versions[this_version]: clust_avg_yhat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9de90-daa7-4714-9e4f-730b9b6bdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_clust_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    clust_avg_yhat = clust_avg_yhat_all[versions[this_version]]\n",
    "    yhat_clust = clust_avg_yhat.copy()\n",
    "    yhat_clust.rename(columns={\"id\": \"cluster_norm\"}, inplace=True)\n",
    "    yhat_clust_all.update({versions[this_version]: yhat_clust})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf3840-039c-4e37-9684-c8328b8bdf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_clust_all[m].cluster_norm.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88d6ea-6a79-4516-95bb-96b2c6114905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for this_version in range(len(versions)):\n",
    "    cluster_features = cluster_features_all[versions[this_version]]\n",
    "    yhat_clust = yhat_clust_all[versions[this_version]]\n",
    "    cluster_features = pd.merge(\n",
    "        cluster_features, \n",
    "        yhat_clust[[\"cluster_norm\", \"ts\", \"yhat\"]],\n",
    "        on=\"cluster_norm\", how=\"left\"\n",
    "    )\n",
    "    cluster_features[\"cluster_norm\"] = cluster_features[\"cluster_norm\"].astype(int)\n",
    "    cluster_features_all.update({versions[this_version]: cluster_features})\n",
    "cluster_features_all[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c7fe4-a20d-43c9-9c68-210c9c32767f",
   "metadata": {},
   "source": [
    "### Cluster - Without Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b0e31-4d44-4beb-b217-19d604b79307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea4c68-1946-4e19-85ab-8d74501d8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clust_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    x_clust = this_df.pivot_table(index=\"id\", columns=\"ts\", values=\"y\").fillna(0)\n",
    "    x_clust_all.update({versions[this_version]: x_clust})\n",
    "\n",
    "x_clust_all[m].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65f011-3ddd-47ee-9d5d-fe4ef22aeff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "clust_model_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    x_clust = x_clust_all[versions[this_version]]\n",
    "    clust_model = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        linkage=\"ward\",\n",
    "        compute_distances=True\n",
    "    )\n",
    "    clust_model = clust_model.fit(x_clust)\n",
    "    clust_model_all.update({versions[this_version]: clust_model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407283ae-2c93-4200-a85d-4ccb204b6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top three levels of the dendrogram\n",
    "plt.figure(figsize=(15,5))\n",
    "plot_dendrogram(clust_model_all[m], truncate_mode=\"level\", p=10)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828b2fa-1f64-4fb1-ae77-489d6b55cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = x_clust_all[m].index\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767eda6-dec0-4b3c-8360-fb7c4ac36a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_average_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    y_sliding = sliding_window_features(this_df, [0,1,2,3])\n",
    "    clust_model = clust_model_all[versions[this_version]]\n",
    "    x_clust = x_clust_all[versions[this_version]]\n",
    "    cluster_average = []\n",
    "    all_ids = x_clust.index\n",
    "    for i in range(n_clusters):\n",
    "        idx = np.where(clust_model.labels_==i)\n",
    "        idx2 = all_ids[idx[0]]\n",
    "\n",
    "        _at = att.loc[att.id.isin(idx2)]\n",
    "        ts = x_clust.loc[idx2].values\n",
    "        t = x_clust.columns\n",
    "        \n",
    "        avg = ts.mean(axis=0)\n",
    "        cluster_average.append(pd.DataFrame({\"ts\": t, \"y\": avg, \"cluster\": i}))\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        for j in range(ts.shape[0]):\n",
    "            plt.plot(t, ts[j])\n",
    "        plt.plot(t, avg, \"k\", linewidth=5)\n",
    "        if ts.shape[0] == 1:\n",
    "            plt.title(\"Cluster {}: ID: {} Version: {}\".format(i, idx[0][0], versions[this_version]))\n",
    "        else:\n",
    "            plt.title(\"Cluster {}: {} Version: {}\".format(i, ts.shape[0],  versions[this_version]))\n",
    "        plt.show()\n",
    "    cluster_average = pd.concat(cluster_average).reset_index(drop=True)\n",
    "    cluster_average_all.update({versions[this_version]: cluster_average})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba57f0-3fb9-4325-9e19-499782ef0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features_actual_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    cluster_features_actual = att.copy()\n",
    "    clust_model = clust_model_all[versions[this_version]]\n",
    "    cluster_features_actual[\"cluster\"] = clust_model.labels_\n",
    "    cluster_features_actual = cluster_features_actual[[\"id\", \"cluster\"]].copy()\n",
    "    cluster_features_actual_all.update({versions[this_version]: cluster_features_actual})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92cd474-f126-4c51-a195-71523c586d6f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "clust_avg_yhat_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    cluster_average = cluster_average_all[versions[this_version]]\n",
    "    clust_avg_yhat = forecast_ets(\n",
    "        cluster_average.rename(columns={\"cluster\": \"id\"}),\n",
    "        all_dates[versions[this_version]],\n",
    "        num_cores=-1,\n",
    "        freq=freq,\n",
    "    )\n",
    "    clust_avg_yhat_all.update({versions[this_version]: clust_avg_yhat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2df80-caf9-4064-91fe-7d6923bb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_clust_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    clust_avg_yhat = clust_avg_yhat_all[versions[this_version]]\n",
    "    yhat_clust = clust_avg_yhat.copy()\n",
    "    yhat_clust.rename(columns={\"id\": \"cluster\"}, inplace=True)\n",
    "    yhat_clust_all.update({versions[this_version]: yhat_clust})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569053f-bc56-413d-85b4-cc0929c0a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_clust_all[m].cluster.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f16e5-fe25-4e5e-be4c-fefc6655379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for this_version in range(len(versions)):\n",
    "    cluster_features_actual = cluster_features_actual_all[versions[this_version]]\n",
    "    yhat_clust = yhat_clust_all[versions[this_version]]\n",
    "    cluster_features_actual = pd.merge(\n",
    "        cluster_features_actual, \n",
    "        yhat_clust[[\"cluster\", \"ts\", \"yhat\"]],\n",
    "        on=\"cluster\", how=\"left\"\n",
    "    )\n",
    "    cluster_features_actual[\"cluster\"] = cluster_features_actual[\"cluster\"].astype(int)\n",
    "    cluster_features_actual_all.update({versions[this_version]: cluster_features_actual})\n",
    "cluster_features_actual_all[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0db3e-503c-4632-9ea1-5d037162ce97",
   "metadata": {},
   "source": [
    "### Final Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320228aa-dc3d-4b96-b4d0-7bceb8d7ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_feature(df, dates, target=\"y\", group=None):\n",
    "    if group is None:\n",
    "        group = [\"id\"]\n",
    "    stats = [\"mean\", \"max\", \"min\", \"std\", \"sum\"]\n",
    "    g = \"_\".join(group)\n",
    "    \n",
    "    train_dates = dates[\"train\"]\n",
    "\n",
    "    df_features = df.loc[df.ts.isin(train_dates)].copy()\n",
    "    df_features = df_features.groupby(group)[target].agg(stats)\n",
    "    df_features.reset_index(inplace=True)\n",
    "    for col in stats:\n",
    "        df_features.rename(columns={col: \"{}_{}_{}\".format(target, g, col)}, inplace=True)\n",
    "        \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fe300-14c1-49cb-a932-fa43ceaef99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_feature(df, all_dates[m], group=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4826dba-a5da-46f4-9afa-11a9df6cd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = y_sliding_all[m][3]\n",
    "aa[aa['id'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7930c-32e5-4b1b-b6f6-33fd68f88c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    df, dates,\n",
    "    yhat_hw, y_sliding, cluster_features_actual, cluster_features,\n",
    "):\n",
    "    from scipy import stats\n",
    "    X = df.copy()\n",
    "    \n",
    "    ## encode time\n",
    "\n",
    "    X[\"month_sin\"] = np.sin(2 * np.pi * X[\"month\"] / 12)\n",
    "    X[\"month_cos\"] = np.cos(2 * np.pi * X[\"month\"] / 12)\n",
    "    \n",
    "    ## summary features at different levels \n",
    "    groups = [\n",
    "        [\"id\"],\n",
    "#         [\"woy\"],\n",
    "#         [\"month\"],\n",
    "#         [\"id\", \"woy\"],\n",
    "#         [\"id\", \"month\"],\n",
    "    ]\n",
    "    for g in groups:\n",
    "        sf = summary_feature(X, dates, group=g)\n",
    "        X = pd.merge(X, sf, on=g, how=\"left\")\n",
    "        \n",
    "    ## holt winters projection\n",
    "    \n",
    "    \n",
    "    X = pd.merge(\n",
    "        X, yhat_hw[[\"id\", \"ts\", \"yhat\"]].rename(columns={\"yhat\": \"y_hw\"}), \n",
    "        on=[\"id\", \"ts\"], how=\"left\"\n",
    "    )    \n",
    "    X[\"next_hw\"] = X[[\"ts\", \"id\", \"y_hw\"]].sort_values(by=[\"id\", \"ts\"]).groupby([\"id\"])[\"y_hw\"].shift(-1)\n",
    "    X[\"prev_hw\"] = X[[\"ts\", \"id\", \"y_hw\"]].sort_values(by=[\"id\", \"ts\"]).groupby([\"id\"])[\"y_hw\"].shift(1)\n",
    "    X[\"next_hw\"] = X[\"next_hw\"].fillna(X[\"y_hw\"])\n",
    "    X[\"prev_hw\"] = X[\"prev_hw\"].fillna(X[\"y_hw\"])\n",
    "    X[\"slope_hw\"] = ((X[\"y_hw\"] - X[\"prev_hw\"]) + ((X[\"next_hw\"] - X[\"prev_hw\"]) / 2)) / 2\n",
    "    \n",
    "    ## differenced of Holt Winters\n",
    "    a = yhat_hw[[\"id\", \"ts\", \"yhat\"]].rename(columns={\"yhat\": \"d1_y_hw\"}).copy()\n",
    "    X_diff = []\n",
    "    all_ids = X.id.unique()\n",
    "    for i in range(len(all_ids)):\n",
    "        id_ = all_ids[i]\n",
    "        tmp = a.loc[a.id == id_].reset_index(drop=True)\n",
    "        tmp2 = tmp.copy()\n",
    "        tmp2[\"d1_y_hw\"] = tmp[\"d1_y_hw\"].diff()\n",
    "        tmp2[\"d1_y_hw\"].fillna(tmp[\"d1_y_hw\"], inplace=True)\n",
    "        X_diff.append(tmp2)\n",
    "    X_diff = pd.concat(X_diff).reset_index(drop=True)\n",
    "    X = pd.merge(\n",
    "        X, X_diff[[\"id\", \"ts\", \"d1_y_hw\"]], \n",
    "        on=[\"id\", \"ts\"], how=\"left\"\n",
    "    )\n",
    "    print(\"base\")\n",
    "    print(X.id.unique())\n",
    "    #print(X.groupby('id')['ts'].min())\n",
    "    ## sliding windows of y\n",
    "    y_sliding['id'] = y_sliding['id'].astype('int64')\n",
    "    X = pd.merge(X, y_sliding.drop(columns=[\"y\"]), on=[\"ts\", \"id\"], how=\"right\")    \n",
    "    print(\"joined sliding\")\n",
    "    print(X.id.unique())\n",
    "    #print(X.groupby('id')['ts'].min())\n",
    "    ## slope estimate of y \n",
    "    lag_cols = [x for x in X.columns if (\"y_lag_\" in x) and (\"div\" not in x)]\n",
    "    mid = len(lag_cols) / 2\n",
    "    mid = lag_cols[int(mid)]\n",
    "    start = lag_cols[0]\n",
    "    end = lag_cols[len(lag_cols) - 1]\n",
    "    X[\"slope_y\"] = ((X[mid] - X[start]) + ((X[end] - X[mid]) / 2)) / 2\n",
    "    \n",
    "    ## ratio of current projection to past lags\n",
    "    for c in lag_cols:\n",
    "        col = \"ratio_hw_to_{}\".format(c.replace(\"y_\", \"\"))\n",
    "        X[col] = X[\"y_hw\"] / X[c]\n",
    "        X[col] = np.nan_to_num(X[col], nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    ## cluster features\n",
    "    X = pd.merge(\n",
    "        X, cluster_features_actual.rename(columns={\"yhat\": \"y_clust_avg\"}), \n",
    "        on=[\"id\", \"ts\"], how=\"left\"\n",
    "    )\n",
    "    print(\"joined cluster feature actual\")\n",
    "    print(X.id.unique())\n",
    "    #print(X.groupby('id')['ts'].min())\n",
    "    X[\"next_clust\"] = X[[\"ts\", \"id\", \"y_clust_avg\"]].sort_values(by=[\"id\", \"ts\"]).groupby([\"id\"])[\"y_clust_avg\"].shift(-1)\n",
    "    X[\"prev_clust\"] = X[[\"ts\", \"id\", \"y_clust_avg\"]].sort_values(by=[\"id\", \"ts\"]).groupby([\"id\"])[\"y_clust_avg\"].shift(1)\n",
    "    X[\"next_clust\"] = X[\"next_clust\"].fillna(X[\"y_clust_avg\"])\n",
    "    X[\"prev_clust\"] = X[\"prev_clust\"].fillna(X[\"y_clust_avg\"])\n",
    "    X[\"slope_clust\"] = ((X[\"y_clust_avg\"] - X[\"prev_clust\"]) + ((X[\"next_clust\"] - X[\"prev_clust\"]) / 2)) / 2\n",
    "    \n",
    "    ## ratio of fitted demand to cluster average\n",
    "    X[\"ratio_hw_to_avg\"] = X[\"y_hw\"] / X[\"y_clust_avg\"]\n",
    "    X[\"ratio_hw_to_avg\"] = np.nan_to_num(X[\"ratio_hw_to_avg\"], nan=0, posinf=0, neginf=0)\n",
    "    lag_cols = [x for x in X.columns if (\"y_lag_\" in x) and (\"div\" not in x)]\n",
    "    for c in lag_cols:\n",
    "        col = \"ratio_hw_to_{}\".format(c.replace(\"y_\", \"\"))\n",
    "        X[col] = X[\"y_hw\"] / X[c]\n",
    "        X[col] = np.nan_to_num(X[col], nan=0, posinf=0, neginf=0)\n",
    "        \n",
    "        col = \"ratio_clust_to_{}\".format(c.replace(\"y_\", \"\"))\n",
    "        X[col] = X[\"y_clust_avg\"] / X[c]\n",
    "        X[col] = np.nan_to_num(X[col], nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    ## cluster features\n",
    "    X = pd.merge(\n",
    "        X, cluster_features.rename(columns={\"yhat\": \"y_clust_avg_norm\"}), \n",
    "        on=[\"id\", \"ts\"], how=\"left\"\n",
    "    )\n",
    "    print(\"joined cluster feature\")\n",
    "    print(X.id.unique())\n",
    "    #print(X.groupby('id')['ts'].min())\n",
    "    X[\"next_clust_norm\"] = X[[\"ts\", \"id\", \"y_clust_avg_norm\"]].sort_values(by=[\"id\", \"ts\"]).groupby([\"id\"])[\"y_clust_avg_norm\"].shift(-1)\n",
    "    X[\"prev_clust_norm\"] = X[[\"ts\", \"id\", \"y_clust_avg_norm\"]].sort_values(by=[\"id\", \"ts\"]).groupby([\"id\"])[\"y_clust_avg_norm\"].shift(1)\n",
    "    X[\"next_clust_norm\"] = X[\"next_clust_norm\"].fillna(X[\"y_clust_avg_norm\"])\n",
    "    X[\"prev_clust_norm\"] = X[\"prev_clust_norm\"].fillna(X[\"y_clust_avg_norm\"])\n",
    "    X[\"slope_clust_norm\"] = ((X[\"y_clust_avg_norm\"] - X[\"prev_clust_norm\"]) + ((X[\"next_clust_norm\"] - X[\"prev_clust_norm\"]) / 2)) / 2\n",
    "    \n",
    "    \n",
    "    X[\"id\"] = X[\"id\"].astype(int)\n",
    "#     X[\"cluster\"] = X[\"cluster\"].astype(int)\n",
    "    \n",
    "    train_dates = dates[\"train\"]\n",
    "    test_dates = dates[\"test\"]\n",
    "    X.to_csv(\"op/all_x.csv\")\n",
    "    train = X.loc[X.ts.isin(train_dates)].reset_index(drop=True)\n",
    "    test = X.loc[X.ts.isin(test_dates)].reset_index(drop=True)\n",
    "    \n",
    "    all_ids_in_train = train.id.unique()\n",
    "    test = test[test['id'].isin(all_ids_in_train)]\n",
    "    \n",
    "    print(X.shape, train.shape, test.shape)\n",
    "    \n",
    "    x_train = train.drop(columns=[\"y\", \"ts\", \"sku\", \"month\", \"woy\"],errors='ignore')\n",
    "    x_test = test.drop(columns=[\"y\", \"ts\", \"sku\", \"month\", \"woy\"],errors='ignore')\n",
    "    y_train = train[\"y\"]\n",
    "    y_test = test[\"y\"]\n",
    "    \n",
    "    return train, test, x_train, x_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13b832-e45a-4549-9758-7c5729dbc20e",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce74792-82cb-4497-b846-fe56a69e2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018a963-0939-4535-8e55-054f810fd370",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12100dc-5ab9-47d3-968b-c0f86ec9753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8aacea-87a2-443b-8484-f967071114ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    print(\"Version {}\".format(versions[this_version]))\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    pred_hw = pred_hws[versions[this_version]]\n",
    "    y_sliding = y_sliding_all[versions[this_version]]\n",
    "    cluster_features_actual = cluster_features_actual_all[versions[this_version]]\n",
    "    cluster_features = cluster_features_all[versions[this_version]]\n",
    "    \n",
    "    models = {}\n",
    "    drop_cols = []\n",
    "    \n",
    "    train, test, x_train, x_test, y_train, y_test = process_data(\n",
    "        df.drop(columns=[\"sku_description\"]), all_dates[versions[this_version]],\n",
    "        pred_hw, y_sliding[2], cluster_features_actual, cluster_features,\n",
    "    )\n",
    "\n",
    "    x_train.drop(columns=drop_cols, inplace=True)\n",
    "    x_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    x_train.dropna(axis=1, inplace=True)\n",
    "    x_test = x_test[x_train.columns]\n",
    "\n",
    "    # x_train.fillna(0, inplace=True)\n",
    "    # x_test.fillna(0, inplace=True)\n",
    "\n",
    "    cols = x_train.select_dtypes(exclude=np.number).columns.tolist() + [\"sku\", \"prod_code\", \"cases\", \"size\"]\n",
    "    encoders = {}\n",
    "    for c in cols:\n",
    "        if c not in x_train.columns:\n",
    "            continue\n",
    "        if c == \"id\":\n",
    "            continue\n",
    "        enc = LabelEncoder()\n",
    "        if \"event\" in c:\n",
    "            enc.fit(pd.concat([x_train[c], x_test[c]]))\n",
    "            x_train[c] = enc.transform(x_train[c])\n",
    "            x_test[c] = enc.transform(x_test[c])\n",
    "        else:\n",
    "            x_train[c] = enc.fit_transform(x_train[c])\n",
    "            x_test[c] = enc.transform(x_test[c])\n",
    "        encoders.update({(versions[this_version], c): enc})\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    yhat_train = model.predict(x_train)\n",
    "    yhat_test = model.predict(x_test)\n",
    "\n",
    "    yhat_train = np.clip(yhat_train, 0, None)\n",
    "    yhat_test = np.clip(yhat_test, 0, None)\n",
    "\n",
    "    models = {\n",
    "        \"version\" : versions[this_version],\n",
    "        \"model\": model,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"x_train\": x_train,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"yhat_train\": yhat_train,\n",
    "        \"yhat_test\": yhat_test,\n",
    "    }\n",
    "    models_all.update({versions[this_version]: models})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917d171-3c2e-4623-897c-d7f6ab7962f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#models_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    models = models_all[versions[this_version]]\n",
    "    k = models[\"model\"]\n",
    "    x_train = models[\"x_train\"]\n",
    "\n",
    "    x_train.dropna(axis=1, inplace=True)\n",
    "\n",
    "    fea_imp = pd.DataFrame({\n",
    "        \"imp\": k.feature_importances_, \n",
    "        \"fea\": x_train.columns\n",
    "    })\n",
    "    fea_imp.sort_values(by=\"imp\", ascending=True, inplace=True)\n",
    "    fea_imp.set_index(\"fea\", inplace=True)\n",
    "    fea_imp.tail(30).plot.barh(figsize=(7,10))\n",
    "    plt.title(\"Version {}\".format(versions[this_version]))\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c917996-92f8-490f-b163-dfaf06fdbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_yhat = []\n",
    "for this_version in range(len(versions)):\n",
    "    models = models_all[versions[this_version]]\n",
    "    aaa = models[\"train\"].copy()\n",
    "    aaa[\"yhat\"] = models[\"yhat_train\"]\n",
    "    aaa['version'] = versions[this_version]\n",
    "    this_rf_yhat = [aaa]\n",
    "\n",
    "    aaa = models[\"test\"].copy()\n",
    "    aaa[\"yhat\"] = models[\"yhat_test\"]\n",
    "    this_rf_yhat.append(aaa)\n",
    "    this_rf_yhat = pd.concat(this_rf_yhat).reset_index(drop=True)\n",
    "    this_rf_yhat['version'] = versions[this_version]\n",
    "    rf_yhat.append(this_rf_yhat)\n",
    "\n",
    "rf_yhat = pd.concat(rf_yhat).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117101a6-03ed-4dd2-b98a-9a89662c55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_version in range(len(versions)):\n",
    "    this_rf_yhat = rf_yhat[rf_yhat['version'] == versions[this_version]]\n",
    "    ts = this_rf_yhat.groupby(\"ts\").sum()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(ts.y, label=\"Actuals\")\n",
    "    plt.plot(ts.yhat, label=\"Forecast main\")\n",
    "    plt.plot(ts.y_hw, label=\"Forecast Holts\")\n",
    "    plt.plot(ts.y_clust_avg, label=\"Forecast Cluster Avg\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c0f2d-683e-45a8-bbd5-741432c8e4f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for this_version in range(len(versions)):\n",
    "    for c in rf_yhat.id.unique():\n",
    "        ts = rf_yhat.loc[ (rf_yhat.id == c) & (rf_yhat.version == versions[this_version])].groupby(\"ts\").sum()\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(ts.y, label=\"Actuals\")\n",
    "        plt.plot(ts.yhat, label=\"Forecast main\")\n",
    "        plt.plot(ts.y_hw, label=\"Forecast Holts\")\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.title(\" ID: {} Version : {}\".format(c,versions[this_version] ))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f56e33-2ac9-4e22-b681-15bd64f6f9c9",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8f1c7-8e85-4c07-b34e-c9a1dbad0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for i in range(len(versions)):\n",
    "    this_ts = rf_yhat[rf_yhat[\"version\"] == versions[i]]\n",
    "    for _id in this_ts.id.unique():\n",
    "        ts = this_ts.loc[(this_ts[\"id\"] == _id)].reset_index(drop=True)\n",
    "        test = ts.loc[ ts.ts.isin( all_dates[versions[i]][\"test\"]) ].copy()\n",
    "        \n",
    "        metrics = calc_metrics(test.y, test.yhat)\n",
    "        metrics.update({\n",
    "            \"id\": _id,\n",
    "            \"version\" : versions[i],\n",
    "            \"sku\": ts[\"sku\"][0],\n",
    "            \"customer\": ts[\"customer\"][0],\n",
    "            \"test_start\": all_dates[versions[i]][\"test\"][0],\n",
    "            \"test_end\": all_dates[versions[i]][\"test\"][-1],\n",
    "            \"total_actual\": test.y.sum(),\n",
    "            \"total_forecast\": test.yhat.sum(),\n",
    "        })\n",
    "        results = results.append(metrics, ignore_index=True, sort=False)\n",
    "\n",
    "main_cols = [\"id\", \"sku\", \"customer\", \"version\", \"test_start\", \"test_end\", \"total_actual\", \"total_forecast\"]\n",
    "results = results[main_cols + list(results.drop(columns=main_cols).columns)].copy()\n",
    "results[\"sp1\"] = results[\"total_actual\"] / results[\"total_forecast\"]\n",
    "results[\"sp1\"] = np.nan_to_num(results[\"sp1\"], nan=1, posinf=0, neginf=0)\n",
    "\n",
    "\n",
    "results.sort_values(by=\"id\").reset_index(drop=True).set_index([\"version\", \"id\"])\n",
    "results.groupby(\"customer\").mean()\n",
    "results.groupby(\"customer\").median()\n",
    "\n",
    "for i in range(len(versions)): \n",
    "    this_results = results[results['version'] == versions[i]]\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.ylim(0,3)\n",
    "    sns.boxplot(data=this_results, y=\"sp1\", x=\"customer\")\n",
    "    plt.title(\"Version {}\".format(versions[i]))\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a919ad-9a67-4437-a041-9821415b9f58",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92e2af-8a00-42f5-91a9-095704a58bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48edd50-6f70-4ac2-bb15-adf9bdf99a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_all = {}\n",
    "fea_imps_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    print(\"Version {}\".format(versions[this_version]))\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    pred_hw = pred_hws[versions[this_version]]\n",
    "    y_sliding = y_sliding_all[versions[this_version]]\n",
    "    cluster_features_actual = cluster_features_actual_all[versions[this_version]]\n",
    "    cluster_features = cluster_features_all[versions[this_version]]\n",
    "    \n",
    "    models = {}\n",
    "    drop_cols = []\n",
    "    fea_imps = {}\n",
    "    \n",
    "    train, test, x_train, x_test, y_train, y_test = process_data(\n",
    "        df.drop(columns=[\"sku_description\"]), all_dates[versions[this_version]],\n",
    "        pred_hw, y_sliding[2], cluster_features_actual, cluster_features,\n",
    "    )\n",
    "\n",
    "    x_train.drop(columns=drop_cols, inplace=True)\n",
    "    x_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    x_train.dropna(axis=1, inplace=True)\n",
    "    x_test = x_test[x_train.columns]\n",
    "\n",
    "    # x_train.fillna(0, inplace=True)\n",
    "    # x_test.fillna(0, inplace=True)\n",
    "\n",
    "    cols = x_train.select_dtypes(exclude=np.number).columns.tolist() + [\"sku\", \"prod_code\", \"cases\", \"size\"]\n",
    "    encoders = {}\n",
    "    for c in cols:\n",
    "        if c not in x_train.columns:\n",
    "            continue\n",
    "        if c == \"id\":\n",
    "            continue\n",
    "        enc = LabelEncoder()\n",
    "        if \"event\" in c:\n",
    "            enc.fit(pd.concat([x_train[c], x_test[c]]))\n",
    "            x_train[c] = enc.transform(x_train[c])\n",
    "            x_test[c] = enc.transform(x_test[c])\n",
    "        else:\n",
    "            x_train[c] = enc.fit_transform(x_train[c])\n",
    "            x_test[c] = enc.transform(x_test[c])\n",
    "        encoders.update({(versions[this_version], c): enc})\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    r = permutation_importance(\n",
    "        model, \n",
    "    #   x_train, y_train,\n",
    "        x_test, y_test,\n",
    "        n_repeats=30,\n",
    "        random_state=123,\n",
    "    )\n",
    "\n",
    "    fea_imps = pd.DataFrame({\n",
    "        \"imp\": r.importances_mean, \n",
    "        \"fea\": x_train.columns,\n",
    "        \"version\":versions[this_version]\n",
    "    })\n",
    "    fea_imps.sort_values(by=\"imp\", ascending=True, inplace=True)\n",
    "    fea_imps.set_index(\"fea\", inplace=True)\n",
    "    fea_imps.tail(50).plot.barh(figsize=(7,10))\n",
    "    plt.show()\n",
    "    fea_imps_all.update({versions[this_version]: fea_imps})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc94c42-5364-4123-a894-4f7176464f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_imps_all[8].to_csv(\"fea_imp_rf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6f043-1bd0-4664-83a8-336a2a4edc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_imps_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b669469-db0f-4c66-8cf0-bdfbe6e9ed86",
   "metadata": {},
   "source": [
    "## Refit without negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3d3fe-39ec-4a37-94b9-cd0034c8e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    fea_imps = fea_imps_all[versions[this_version]]\n",
    "    fea_imps.drop(columns={'version'},inplace=True)\n",
    "    thres = np.quantile(fea_imps.imp, 0.1)\n",
    "    thres = 0\n",
    "    print(thres)\n",
    "    fea_imps.loc[fea_imps.imp < thres].shape, fea_imps.loc[fea_imps.imp >= thres].shape\n",
    "    drop_cols = fea_imps.loc[(fea_imps.imp < thres)].index.values\n",
    "    print(drop_cols)\n",
    "    print(\"Version {}\".format(versions[this_version]))\n",
    "    this_df = df[df['ts'] < np.min(all_dates[versions[this_version]]['forecast'])]\n",
    "    pred_hw = pred_hws[versions[this_version]]\n",
    "    y_sliding = y_sliding_all[versions[this_version]]\n",
    "    cluster_features_actual = cluster_features_actual_all[versions[this_version]]\n",
    "    cluster_features = cluster_features_all[versions[this_version]]\n",
    "    \n",
    "    models = {}\n",
    "    train, test, x_train, x_test, y_train, y_test = process_data(\n",
    "        df.drop(columns=[\"sku_description\"]), all_dates[versions[this_version]],\n",
    "        pred_hw, y_sliding[2], cluster_features_actual, cluster_features,\n",
    "    )\n",
    "\n",
    "    x_train.drop(columns=drop_cols, inplace=True)\n",
    "    x_test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    x_train.dropna(axis=1, inplace=True)\n",
    "    x_test = x_test[x_train.columns]\n",
    "\n",
    "    # x_train.fillna(0, inplace=True)\n",
    "    # x_test.fillna(0, inplace=True)\n",
    "\n",
    "    cols = x_train.select_dtypes(exclude=np.number).columns.tolist() + [\"sku\", \"prod_code\", \"cases\", \"size\"]\n",
    "    encoders = {}\n",
    "    for c in cols:\n",
    "        if c not in x_train.columns:\n",
    "            continue\n",
    "        if c == \"id\":\n",
    "            continue\n",
    "        enc = LabelEncoder()\n",
    "        if \"event\" in c:\n",
    "            enc.fit(pd.concat([x_train[c], x_test[c]]))\n",
    "            x_train[c] = enc.transform(x_train[c])\n",
    "            x_test[c] = enc.transform(x_test[c])\n",
    "        else:\n",
    "            x_train[c] = enc.fit_transform(x_train[c])\n",
    "            x_test[c] = enc.transform(x_test[c])\n",
    "        encoders.update({(versions[this_version], c): enc})\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    yhat_train = model.predict(x_train)\n",
    "    yhat_test = model.predict(x_test)\n",
    "\n",
    "    yhat_train = np.clip(yhat_train, 0, None)\n",
    "    yhat_test = np.clip(yhat_test, 0, None)\n",
    "\n",
    "    models = {\n",
    "        \"version\" : versions[this_version],\n",
    "        \"model\": model,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"x_train\": x_train,\n",
    "        \"x_test\": x_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"yhat_train\": yhat_train,\n",
    "        \"yhat_test\": yhat_test,\n",
    "    }\n",
    "    models_all.update({versions[this_version]: models})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d56387-4dff-4957-b269-bfd24535dba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#models_all = {}\n",
    "for this_version in range(len(versions)):\n",
    "    models = models_all[versions[this_version]]\n",
    "    k = models[\"model\"]\n",
    "    x_train = models[\"x_train\"]\n",
    "\n",
    "    x_train.dropna(axis=1, inplace=True)\n",
    "\n",
    "    fea_imp = pd.DataFrame({\n",
    "        \"imp\": k.feature_importances_, \n",
    "        \"fea\": x_train.columns\n",
    "    })\n",
    "    fea_imp.sort_values(by=\"imp\", ascending=True, inplace=True)\n",
    "    fea_imp.set_index(\"fea\", inplace=True)\n",
    "    fea_imp.tail(30).plot.barh(figsize=(7,10))\n",
    "    plt.title(\"Version {}\".format(versions[this_version]))\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24380370-ce44-40ef-bd35-c21932a73563",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_yhat = []\n",
    "for this_version in range(len(versions)):\n",
    "    models = models_all[versions[this_version]]\n",
    "    aaa = models[\"train\"].copy()\n",
    "    aaa[\"yhat\"] = models[\"yhat_train\"]\n",
    "    aaa['version'] = versions[this_version]\n",
    "    this_rf_yhat = [aaa]\n",
    "\n",
    "    aaa = models[\"test\"].copy()\n",
    "    aaa[\"yhat\"] = models[\"yhat_test\"]\n",
    "    this_rf_yhat.append(aaa)\n",
    "    this_rf_yhat = pd.concat(this_rf_yhat).reset_index(drop=True)\n",
    "    this_rf_yhat['version'] = versions[this_version]\n",
    "    rf_yhat.append(this_rf_yhat)\n",
    "\n",
    "rf_yhat = pd.concat(rf_yhat).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e556bc3-e606-4bc6-a631-9af609fad56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_version in range(len(versions)):\n",
    "    this_rf_yhat = rf_yhat[rf_yhat['version'] == versions[this_version]]\n",
    "    ts = this_rf_yhat.groupby(\"ts\").sum()\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(ts.y, label=\"Actuals\")\n",
    "    plt.plot(ts.yhat, label=\"Forecast main\")\n",
    "    plt.plot(ts.y_hw, label=\"Forecast Holts\")\n",
    "    plt.plot(ts.y_clust_avg, label=\"Forecast Cluster Avg\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70457db8-c998-4c69-87ca-996658bcc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_version in range(len(versions)):\n",
    "    for c in rf_yhat.id.unique():\n",
    "        ts = rf_yhat.loc[ (rf_yhat.id == c) & (rf_yhat.version == versions[this_version])].groupby(\"ts\").sum()\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(ts.y, label=\"Actuals\")\n",
    "        plt.plot(ts.yhat, label=\"Forecast main\")\n",
    "        plt.plot(ts.y_hw, label=\"Forecast Holts\")\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.title(\" ID: {} Version : {}\".format(c,versions[this_version] ))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697675d3-4210-4233-893a-f27a9f461543",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb338d83-4509-451e-b107-87abab673570",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "for i in range(len(versions)):\n",
    "    this_ts = rf_yhat[rf_yhat[\"version\"] == versions[i]]\n",
    "    for _id in this_ts.id.unique():\n",
    "        ts = this_ts.loc[(this_ts[\"id\"] == _id)].reset_index(drop=True)\n",
    "        test = ts.loc[ ts.ts.isin( all_dates[versions[i]][\"test\"]) ].copy()\n",
    "        \n",
    "        metrics = calc_metrics(test.y, test.yhat)\n",
    "        metrics.update({\n",
    "            \"id\": _id,\n",
    "            \"version\" : versions[i],\n",
    "            \"sku\": ts[\"sku\"][0],\n",
    "            \"customer\": ts[\"customer\"][0],\n",
    "            \"test_start\": all_dates[versions[i]][\"test\"][0],\n",
    "            \"test_end\": all_dates[versions[i]][\"test\"][-1],\n",
    "            \"total_actual\": test.y.sum(),\n",
    "            \"total_forecast\": test.yhat.sum(),\n",
    "        })\n",
    "        results = results.append(metrics, ignore_index=True, sort=False)\n",
    "\n",
    "main_cols = [\"id\", \"sku\", \"customer\", \"version\", \"test_start\", \"test_end\", \"total_actual\", \"total_forecast\"]\n",
    "results = results[main_cols + list(results.drop(columns=main_cols).columns)].copy()\n",
    "results[\"sp1\"] = results[\"total_actual\"] / results[\"total_forecast\"]\n",
    "results[\"sp1\"] = np.nan_to_num(results[\"sp1\"], nan=1, posinf=0, neginf=0)\n",
    "\n",
    "\n",
    "results.sort_values(by=\"id\").reset_index(drop=True).set_index([\"version\", \"id\"])\n",
    "results.groupby(\"customer\").mean()\n",
    "results.groupby(\"customer\").median()\n",
    "\n",
    "for i in range(len(versions)): \n",
    "    this_results = results[results['version'] == versions[i]]\n",
    "    plt.figure(figsize=(5,2))\n",
    "    plt.ylim(0,3)\n",
    "    sns.boxplot(data=this_results, y=\"sp1\", x=\"customer\")\n",
    "    plt.title(\"Version {}\".format(versions[i]))\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461f9ff-f7b2-4bff-aa51-7ef222576b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.ylim(0,3)\n",
    "sns.boxplot(data=results, y=\"sp1\", x=\"customer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55fce6f-34e7-4776-8d02-2a46d35d29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean(), results.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e37d53-ed61-4466-8880-43abec0d708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.merge(att[[\"id\", \"sku_code\"]], rf_yhat, on=[\"id\"], how=\"right\")\n",
    "# print(a.shape, rf_yhat.shape)\n",
    "# a.to_csv(\"prediction_sellout_rf_v1_0819.csv\", index=False)\n",
    "# a.to_parquet(\"prediction_sellout_rf_v1_0819.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc8852-ccdd-4c86-90b8-70e0a1e4bf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b639a1f-fd2b-44be-a530-f31e2d8819ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a218f-d47c-4ad2-aa5c-a9140e83b32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271d9fe-5b9a-4757-b9b6-95629768f8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694fad2-44db-4f2d-80ca-78d991bb10bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
